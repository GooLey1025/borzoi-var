sequence: [(None, 524288, 4)] -> [(None, 524288, 4)]
stochastic_reverse_complement: (None, 524288, 4) -> ((None, 524288, 4), ())
stochastic_shift: (None, 524288, 4) -> (None, 524288, 4)
conv1d: (None, 524288, 4) -> (None, 524288, 512)
max_pooling1d: (None, 524288, 512) -> (None, 262144, 512)
batch_normalization: (None, 262144, 512) -> (None, 262144, 512)
tf.nn.gelu: (None, 262144, 512) -> (None, 262144, 512)
conv1d_1: (None, 262144, 512) -> (None, 262144, 608)
max_pooling1d_1: (None, 262144, 608) -> (None, 131072, 608)
batch_normalization_1: (None, 131072, 608) -> (None, 131072, 608)
tf.nn.gelu_1: (None, 131072, 608) -> (None, 131072, 608)
conv1d_2: (None, 131072, 608) -> (None, 131072, 736)
max_pooling1d_2: (None, 131072, 736) -> (None, 65536, 736)
batch_normalization_2: (None, 65536, 736) -> (None, 65536, 736)
tf.nn.gelu_2: (None, 65536, 736) -> (None, 65536, 736)
conv1d_3: (None, 65536, 736) -> (None, 65536, 896)
max_pooling1d_3: (None, 65536, 896) -> (None, 32768, 896)
batch_normalization_3: (None, 32768, 896) -> (None, 32768, 896)
tf.nn.gelu_3: (None, 32768, 896) -> (None, 32768, 896)
conv1d_4: (None, 32768, 896) -> (None, 32768, 1056)
max_pooling1d_4: (None, 32768, 1056) -> (None, 16384, 1056)
batch_normalization_4: (None, 16384, 1056) -> (None, 16384, 1056)
tf.nn.gelu_4: (None, 16384, 1056) -> (None, 16384, 1056)
conv1d_5: (None, 16384, 1056) -> (None, 16384, 1280)
max_pooling1d_5: (None, 16384, 1280) -> (None, 8192, 1280)
batch_normalization_5: (None, 8192, 1280) -> (None, 8192, 1280)
tf.nn.gelu_5: (None, 8192, 1280) -> (None, 8192, 1280)
conv1d_6: (None, 8192, 1280) -> (None, 8192, 1536)
max_pooling1d_6: (None, 8192, 1536) -> (None, 4096, 1536)
layer_normalization: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention: (None, 4096, 1536) -> (None, 4096, 1536)
dropout: (None, 4096, 1536) -> (None, 4096, 1536)
add: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_1: (None, 4096, 1536) -> (None, 4096, 1536)
dense: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_1: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_1: (None, 4096, 3072) -> (None, 4096, 3072)
dense_1: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_2: (None, 4096, 1536) -> (None, 4096, 1536)
add_1: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_2: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention_1: (None, 4096, 1536) -> (None, 4096, 1536)
dropout_3: (None, 4096, 1536) -> (None, 4096, 1536)
add_2: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_3: (None, 4096, 1536) -> (None, 4096, 1536)
dense_2: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_4: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_2: (None, 4096, 3072) -> (None, 4096, 3072)
dense_3: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_5: (None, 4096, 1536) -> (None, 4096, 1536)
add_3: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_4: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention_2: (None, 4096, 1536) -> (None, 4096, 1536)
dropout_6: (None, 4096, 1536) -> (None, 4096, 1536)
add_4: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_5: (None, 4096, 1536) -> (None, 4096, 1536)
dense_4: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_7: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_3: (None, 4096, 3072) -> (None, 4096, 3072)
dense_5: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_8: (None, 4096, 1536) -> (None, 4096, 1536)
add_5: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_6: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention_3: (None, 4096, 1536) -> (None, 4096, 1536)
dropout_9: (None, 4096, 1536) -> (None, 4096, 1536)
add_6: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_7: (None, 4096, 1536) -> (None, 4096, 1536)
dense_6: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_10: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_4: (None, 4096, 3072) -> (None, 4096, 3072)
dense_7: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_11: (None, 4096, 1536) -> (None, 4096, 1536)
add_7: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_8: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention_4: (None, 4096, 1536) -> (None, 4096, 1536)
dropout_12: (None, 4096, 1536) -> (None, 4096, 1536)
add_8: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_9: (None, 4096, 1536) -> (None, 4096, 1536)
dense_8: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_13: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_5: (None, 4096, 3072) -> (None, 4096, 3072)
dense_9: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_14: (None, 4096, 1536) -> (None, 4096, 1536)
add_9: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_10: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention_5: (None, 4096, 1536) -> (None, 4096, 1536)
dropout_15: (None, 4096, 1536) -> (None, 4096, 1536)
add_10: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_11: (None, 4096, 1536) -> (None, 4096, 1536)
dense_10: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_16: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_6: (None, 4096, 3072) -> (None, 4096, 3072)
dense_11: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_17: (None, 4096, 1536) -> (None, 4096, 1536)
add_11: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_12: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention_6: (None, 4096, 1536) -> (None, 4096, 1536)
dropout_18: (None, 4096, 1536) -> (None, 4096, 1536)
add_12: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_13: (None, 4096, 1536) -> (None, 4096, 1536)
dense_12: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_19: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_7: (None, 4096, 3072) -> (None, 4096, 3072)
dense_13: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_20: (None, 4096, 1536) -> (None, 4096, 1536)
add_13: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_14: (None, 4096, 1536) -> (None, 4096, 1536)
multihead_attention_7: (None, 4096, 1536) -> (None, 4096, 1536)
dropout_21: (None, 4096, 1536) -> (None, 4096, 1536)
add_14: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
layer_normalization_15: (None, 4096, 1536) -> (None, 4096, 1536)
dense_14: (None, 4096, 1536) -> (None, 4096, 3072)
dropout_22: (None, 4096, 3072) -> (None, 4096, 3072)
re_lu_8: (None, 4096, 3072) -> (None, 4096, 3072)
dense_15: (None, 4096, 3072) -> (None, 4096, 1536)
dropout_23: (None, 4096, 1536) -> (None, 4096, 1536)
add_15: [(None, 4096, 1536), (None, 4096, 1536)] -> (None, 4096, 1536)
batch_normalization_6: (None, 4096, 1536) -> (None, 4096, 1536)
tf.nn.gelu_6: (None, 4096, 1536) -> (None, 4096, 1536)
batch_normalization_7: (None, 8192, 1536) -> (None, 8192, 1536)
dense_16: (None, 4096, 1536) -> (None, 4096, 1536)
tf.nn.gelu_7: (None, 8192, 1536) -> (None, 8192, 1536)
up_sampling1d: (None, 4096, 1536) -> (None, 8192, 1536)
dense_17: (None, 8192, 1536) -> (None, 8192, 1536)
add_16: [(None, 8192, 1536), (None, 8192, 1536)] -> (None, 8192, 1536)
separable_conv1d: (None, 8192, 1536) -> (None, 8192, 1536)
batch_normalization_8: (None, 8192, 1536) -> (None, 8192, 1536)
tf.nn.gelu_8: (None, 8192, 1536) -> (None, 8192, 1536)
batch_normalization_9: (None, 16384, 1280) -> (None, 16384, 1280)
dense_18: (None, 8192, 1536) -> (None, 8192, 1536)
tf.nn.gelu_9: (None, 16384, 1280) -> (None, 16384, 1280)
up_sampling1d_1: (None, 8192, 1536) -> (None, 16384, 1536)
dense_19: (None, 16384, 1280) -> (None, 16384, 1536)
add_17: [(None, 16384, 1536), (None, 16384, 1536)] -> (None, 16384, 1536)
separable_conv1d_1: (None, 16384, 1536) -> (None, 16384, 1536)
cropping1d: (None, 16384, 1536) -> (None, 16352, 1536)
batch_normalization_10: (None, 16352, 1536) -> (None, 16352, 1536)
tf.nn.gelu_10: (None, 16352, 1536) -> (None, 16352, 1536)
conv1d_7: (None, 16352, 1536) -> (None, 16352, 1920)
dropout_24: (None, 16352, 1920) -> (None, 16352, 1920)
tf.nn.gelu_11: (None, 16352, 1920) -> (None, 16352, 1920)
dense_20: (None, 16352, 1920) -> (None, 16352, 7611)
switch_reverse: [(None, 16352, 7611), ()] -> (None, 16352, 7611)
